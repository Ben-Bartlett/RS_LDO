## Initial

This folder covers going from the original bag files and bag file viewer to an updated version dealing with the colour stream correctly for OpenCV and trialing the RealSense Python Wrapper Examples.

This folder also is a good place to start to make sure the Repo is setup and the camera is working with a test script that streams the RGB, Depth and 2 IR camera streams.

Further details available in the README.md in this [folder](https://github.com/Ben-Bartlett/RS_LDO/tree/main/Code/Initial)



## Development

This folder covers the main development process of segmenting the RGB stream to detect the cable. Generating and Manipulating Point clouds generated by the sensor and Creating virtual cables within the point cloud data, to complete and estimate the position of the cable over time. 

For a number of camera functions and code examples, there is some good functionality within some fo the files, as well as troubleshooting and workarounds visible in different cases. 


Further details available in the README.md in this folder



## Final

This folder will be kept up to date but as of now, the second last meeting of the placement led to a few key points.

1. The visualisations were created for verification and validation of process, as well as showing the work completed. This is not needed in the end and as such should increase the processing speed. 

2. On processing speed the criteria of 60 FPS has been set. Some speed, but in theory well doable. As such the bag files will need to be recorded at 60FPS, which is the max for the RGB in theory based on the realsense-viewer settings. The depth can go much higher but without RGB we lose all segmenation of the pointcloud with regards to the "rope".

3. The understanding of the point-to-point mapping criteria was incorrect. Some confusion arose be it due to unfamiliarity, translation and notation but it seems to be clear now nearing the end of the project. While we know in a pointcloud, even of a static object, the points will vary from frame to frame. As such we need to determine "features" or actual frame to frame mappings. This is achievable with the splines created. Given the splines are created from real input data we can look at the following:

___________
___________
<br></br>
*Given two splines at times **T<sub>1</sub>** and **T<sub>2</sub>**, we want to determine the difference between any equivalent points along the splines. <br></br>
Let:*

   - ***P<sub>1</sub>(&alpha;)** = [X<sub>1</sub>(&alpha;); Y<sub>1</sub>(&alpha;); Z<sub>1</sub>(&alpha;)] be the spline at time T<sub>1</sub>.*

   - ***P<sub>2</sub>(&alpha;)** = [X<sub>2</sub>(&alpha;); Y<sub>2</sub>(&alpha;); Z<sub>2</sub>(&alpha;)] be the spline at time T<sub>2</sub>.*

*Here, **(&alpha;)** ranges from 0 to 1 and represents a percentage along the spline.*

<br></br>
*To determine the difference between equivalent points on the splines at any **(&alpha;)**, we compute the difference vector:*

   - ***&Delta; P(&alpha;)** = P<sub>2</sub>(&alpha;) - P<sub>1</sub>(&alpha;)*

<br></br>
*This results in:*

   - ***&Delta; P(&alpha;)** = [X<sub>2</sub>(&alpha;) - X<sub>1</sub>(&alpha;); Y<sub>2</sub>(&alpha;) - Y<sub>1</sub>(&alpha;); Z<sub>2</sub>(&alpha;) - Z<sub>1</sub>(&alpha;)]*

*This difference vector **&Delta; P(&alpha;)** represents the change in position of equivalent points along the splines from time **T<sub>1</sub>** to time **T<sub>2</sub>**.*



<sub>@Paddy sound</sub>
<br></br>
______________
______________

- The file seg\_tuning_\bag.py was mentioned elsewhere and linked, but it will allow you to choose the Red pixels to save an LAB colour to txt file and you can also adjust the canny thresholds "preferably" to a noisy low level but you can adjust as needed. 

- The file final.py is the "final" file producing the desired output of point 3. above. GPU vector operations are utilized for maximum speed leading to a 60 FPS bagfile recording staying at 60 FPS while producing the desired results for X points

